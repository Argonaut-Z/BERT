[2025-03-03 09:20:16] - INFO: 成功导入BERT配置文件 /root/epfs/bert_base_chinese/config.json
[2025-03-03 09:20:16] - INFO:  ### 将当前配置打印到日志文件中 
[2025-03-03 09:20:16] - INFO: ### project_dir = /root/epfs
[2025-03-03 09:20:16] - INFO: ### dataset_dir = /root/epfs/data/SongCi
[2025-03-03 09:20:16] - INFO: ### pretrained_model_dir = /root/epfs/bert_base_chinese
[2025-03-03 09:20:16] - INFO: ### train_file_path = /root/epfs/data/SongCi/songci.train.txt
[2025-03-03 09:20:16] - INFO: ### val_file_path = /root/epfs/data/SongCi/songci.valid.txt
[2025-03-03 09:20:16] - INFO: ### test_file_path = /root/epfs/data/SongCi/songci.test.txt
[2025-03-03 09:20:16] - INFO: ### data_name = songci
[2025-03-03 09:20:16] - INFO: ### vocab_path = /root/epfs/bert_base_chinese/vocab.txt
[2025-03-03 09:20:16] - INFO: ### device = cuda:0
[2025-03-03 09:20:16] - INFO: ### model_save_dir = /root/epfs/cache
[2025-03-03 09:20:16] - INFO: ### logs_save_dir = /root/epfs/logs
[2025-03-03 09:20:16] - INFO: ### model_save_path = /root/epfs/cache/model_songci.bin
[2025-03-03 09:20:16] - INFO: ### writer = <torch.utils.tensorboard.writer.SummaryWriter object at 0x7fa1e0b06470>
[2025-03-03 09:20:16] - INFO: ### is_sample_shuffle = True
[2025-03-03 09:20:16] - INFO: ### use_embedding_weight = True
[2025-03-03 09:20:16] - INFO: ### batch_size = 16
[2025-03-03 09:20:16] - INFO: ### max_sen_len = None
[2025-03-03 09:20:16] - INFO: ### pad_index = 0
[2025-03-03 09:20:16] - INFO: ### random_state = 2022
[2025-03-03 09:20:16] - INFO: ### learning_rate = 4e-05
[2025-03-03 09:20:16] - INFO: ### weight_decay = 0.1
[2025-03-03 09:20:16] - INFO: ### masked_rate = 0.15
[2025-03-03 09:20:16] - INFO: ### masked_token_rate = 0.8
[2025-03-03 09:20:16] - INFO: ### masked_token_unchanged_rate = 0.5
[2025-03-03 09:20:16] - INFO: ### log_level = 10
[2025-03-03 09:20:16] - INFO: ### use_torch_multi_head = False
[2025-03-03 09:20:16] - INFO: ### epochs = 100
[2025-03-03 09:20:16] - INFO: ### model_val_per_epoch = 1
[2025-03-03 09:20:16] - INFO: ### vocab_size = 21128
[2025-03-03 09:20:16] - INFO: ### hidden_size = 768
[2025-03-03 09:20:16] - INFO: ### num_hidden_layers = 12
[2025-03-03 09:20:16] - INFO: ### num_attention_heads = 12
[2025-03-03 09:20:16] - INFO: ### hidden_act = gelu
[2025-03-03 09:20:16] - INFO: ### intermediate_size = 3072
[2025-03-03 09:20:16] - INFO: ### pad_token_id = 0
[2025-03-03 09:20:16] - INFO: ### hidden_dropout_prob = 0.1
[2025-03-03 09:20:16] - INFO: ### attention_probs_dropout_prob = 0.1
[2025-03-03 09:20:16] - INFO: ### max_position_embeddings = 512
[2025-03-03 09:20:16] - INFO: ### type_vocab_size = 2
[2025-03-03 09:20:16] - INFO: ### initializer_range = 0.02
[2025-03-03 09:20:16] - INFO: ### architectures = ['BertForMaskedLM']
[2025-03-03 09:20:16] - INFO: ### directionality = bidi
[2025-03-03 09:20:16] - INFO: ### layer_norm_eps = 1e-12
[2025-03-03 09:20:16] - INFO: ### model_type = bert
[2025-03-03 09:20:16] - INFO: ### pooler_fc_size = 768
[2025-03-03 09:20:16] - INFO: ### pooler_num_attention_heads = 12
[2025-03-03 09:20:16] - INFO: ### pooler_num_fc_layers = 3
[2025-03-03 09:20:16] - INFO: ### pooler_size_per_head = 128
[2025-03-03 09:20:16] - INFO: ### pooler_type = first_token_transform
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.embeddings.word_embeddings.weight赋值给bert_embeddings.word_embeddings.embedding.weight,参数形状为:torch.Size([21128, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.embeddings.position_embeddings.weight赋值给bert_embeddings.position_embeddings.embedding.weight,参数形状为:torch.Size([512, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.embeddings.token_type_embeddings.weight赋值给bert_embeddings.token_type_embeddings.embedding.weight,参数形状为:torch.Size([2, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.embeddings.LayerNorm.gamma赋值给bert_embeddings.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.embeddings.LayerNorm.beta赋值给bert_embeddings.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.self.query.weight赋值给bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.self.query.bias赋值给bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.self.key.weight赋值给bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.self.key.bias赋值给bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.self.value.weight赋值给bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.self.value.bias赋值给bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.output.dense.weight赋值给bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.output.dense.bias赋值给bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.0.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.output.LayerNorm.beta赋值给bert_encoder.bert_layers.0.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.0.intermediate.dense.weight赋值给bert_encoder.bert_layers.0.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.0.intermediate.dense.bias赋值给bert_encoder.bert_layers.0.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.0.output.dense.weight赋值给bert_encoder.bert_layers.0.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.0.output.dense.bias赋值给bert_encoder.bert_layers.0.bert_output.dense.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.0.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.0.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.0.output.LayerNorm.beta赋值给bert_encoder.bert_layers.0.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.self.query.weight赋值给bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.self.query.bias赋值给bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.self.key.weight赋值给bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.self.key.bias赋值给bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.self.value.weight赋值给bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.self.value.bias赋值给bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.output.dense.weight赋值给bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.output.dense.bias赋值给bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.1.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.output.LayerNorm.beta赋值给bert_encoder.bert_layers.1.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.1.intermediate.dense.weight赋值给bert_encoder.bert_layers.1.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.1.intermediate.dense.bias赋值给bert_encoder.bert_layers.1.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.1.output.dense.weight赋值给bert_encoder.bert_layers.1.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.1.output.dense.bias赋值给bert_encoder.bert_layers.1.bert_output.dense.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.1.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.1.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.1.output.LayerNorm.beta赋值给bert_encoder.bert_layers.1.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.self.query.weight赋值给bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.self.query.bias赋值给bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.self.key.weight赋值给bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.self.key.bias赋值给bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.self.value.weight赋值给bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.self.value.bias赋值给bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.output.dense.weight赋值给bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.output.dense.bias赋值给bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.2.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.output.LayerNorm.beta赋值给bert_encoder.bert_layers.2.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.2.intermediate.dense.weight赋值给bert_encoder.bert_layers.2.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.2.intermediate.dense.bias赋值给bert_encoder.bert_layers.2.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.2.output.dense.weight赋值给bert_encoder.bert_layers.2.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.2.output.dense.bias赋值给bert_encoder.bert_layers.2.bert_output.dense.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.2.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.2.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.2.output.LayerNorm.beta赋值给bert_encoder.bert_layers.2.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.self.query.weight赋值给bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.self.query.bias赋值给bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.self.key.weight赋值给bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.self.key.bias赋值给bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.self.value.weight赋值给bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.self.value.bias赋值给bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.output.dense.weight赋值给bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.output.dense.bias赋值给bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.3.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.output.LayerNorm.beta赋值给bert_encoder.bert_layers.3.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.3.intermediate.dense.weight赋值给bert_encoder.bert_layers.3.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.3.intermediate.dense.bias赋值给bert_encoder.bert_layers.3.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.3.output.dense.weight赋值给bert_encoder.bert_layers.3.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.3.output.dense.bias赋值给bert_encoder.bert_layers.3.bert_output.dense.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.3.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.3.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.3.output.LayerNorm.beta赋值给bert_encoder.bert_layers.3.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.self.query.weight赋值给bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.self.query.bias赋值给bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.self.key.weight赋值给bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.self.key.bias赋值给bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.self.value.weight赋值给bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.self.value.bias赋值给bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.output.dense.weight赋值给bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.output.dense.bias赋值给bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.4.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.output.LayerNorm.beta赋值给bert_encoder.bert_layers.4.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.4.intermediate.dense.weight赋值给bert_encoder.bert_layers.4.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.4.intermediate.dense.bias赋值给bert_encoder.bert_layers.4.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.4.output.dense.weight赋值给bert_encoder.bert_layers.4.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.4.output.dense.bias赋值给bert_encoder.bert_layers.4.bert_output.dense.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.4.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.4.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.4.output.LayerNorm.beta赋值给bert_encoder.bert_layers.4.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.self.query.weight赋值给bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.self.query.bias赋值给bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.self.key.weight赋值给bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.self.key.bias赋值给bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.self.value.weight赋值给bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.self.value.bias赋值给bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.output.dense.weight赋值给bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.output.dense.bias赋值给bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.5.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.output.LayerNorm.beta赋值给bert_encoder.bert_layers.5.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.5.intermediate.dense.weight赋值给bert_encoder.bert_layers.5.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.5.intermediate.dense.bias赋值给bert_encoder.bert_layers.5.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.5.output.dense.weight赋值给bert_encoder.bert_layers.5.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.5.output.dense.bias赋值给bert_encoder.bert_layers.5.bert_output.dense.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.5.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.5.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.5.output.LayerNorm.beta赋值给bert_encoder.bert_layers.5.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.self.query.weight赋值给bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.self.query.bias赋值给bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.self.key.weight赋值给bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.self.key.bias赋值给bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.self.value.weight赋值给bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.self.value.bias赋值给bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.output.dense.weight赋值给bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.output.dense.bias赋值给bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.6.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.output.LayerNorm.beta赋值给bert_encoder.bert_layers.6.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.6.intermediate.dense.weight赋值给bert_encoder.bert_layers.6.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.6.intermediate.dense.bias赋值给bert_encoder.bert_layers.6.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.6.output.dense.weight赋值给bert_encoder.bert_layers.6.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.6.output.dense.bias赋值给bert_encoder.bert_layers.6.bert_output.dense.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.6.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.6.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.6.output.LayerNorm.beta赋值给bert_encoder.bert_layers.6.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.self.query.weight赋值给bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.self.query.bias赋值给bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.self.key.weight赋值给bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.self.key.bias赋值给bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.self.value.weight赋值给bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.self.value.bias赋值给bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.output.dense.weight赋值给bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.output.dense.bias赋值给bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.7.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.output.LayerNorm.beta赋值给bert_encoder.bert_layers.7.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.7.intermediate.dense.weight赋值给bert_encoder.bert_layers.7.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.7.intermediate.dense.bias赋值给bert_encoder.bert_layers.7.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.7.output.dense.weight赋值给bert_encoder.bert_layers.7.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.7.output.dense.bias赋值给bert_encoder.bert_layers.7.bert_output.dense.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.7.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.7.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.7.output.LayerNorm.beta赋值给bert_encoder.bert_layers.7.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.self.query.weight赋值给bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.self.query.bias赋值给bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.self.key.weight赋值给bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.self.key.bias赋值给bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.self.value.weight赋值给bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.self.value.bias赋值给bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.output.dense.weight赋值给bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.output.dense.bias赋值给bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.8.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.output.LayerNorm.beta赋值给bert_encoder.bert_layers.8.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.8.intermediate.dense.weight赋值给bert_encoder.bert_layers.8.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.8.intermediate.dense.bias赋值给bert_encoder.bert_layers.8.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.8.output.dense.weight赋值给bert_encoder.bert_layers.8.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.8.output.dense.bias赋值给bert_encoder.bert_layers.8.bert_output.dense.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.8.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.8.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.8.output.LayerNorm.beta赋值给bert_encoder.bert_layers.8.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.self.query.weight赋值给bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.self.query.bias赋值给bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.self.key.weight赋值给bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.self.key.bias赋值给bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.self.value.weight赋值给bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.self.value.bias赋值给bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.output.dense.weight赋值给bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.output.dense.bias赋值给bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.9.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.output.LayerNorm.beta赋值给bert_encoder.bert_layers.9.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.9.intermediate.dense.weight赋值给bert_encoder.bert_layers.9.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.9.intermediate.dense.bias赋值给bert_encoder.bert_layers.9.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.9.output.dense.weight赋值给bert_encoder.bert_layers.9.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.9.output.dense.bias赋值给bert_encoder.bert_layers.9.bert_output.dense.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.9.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.9.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.9.output.LayerNorm.beta赋值给bert_encoder.bert_layers.9.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.self.query.weight赋值给bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.self.query.bias赋值给bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.self.key.weight赋值给bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.self.key.bias赋值给bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.self.value.weight赋值给bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.self.value.bias赋值给bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.output.dense.weight赋值给bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.output.dense.bias赋值给bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.10.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.output.LayerNorm.beta赋值给bert_encoder.bert_layers.10.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.10.intermediate.dense.weight赋值给bert_encoder.bert_layers.10.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.10.intermediate.dense.bias赋值给bert_encoder.bert_layers.10.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.10.output.dense.weight赋值给bert_encoder.bert_layers.10.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.10.output.dense.bias赋值给bert_encoder.bert_layers.10.bert_output.dense.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.10.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.10.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.10.output.LayerNorm.beta赋值给bert_encoder.bert_layers.10.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.self.query.weight赋值给bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.self.query.bias赋值给bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.self.key.weight赋值给bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.self.key.bias赋值给bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.self.value.weight赋值给bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.self.value.bias赋值给bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.output.dense.weight赋值给bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.output.dense.bias赋值给bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.11.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.output.LayerNorm.beta赋值给bert_encoder.bert_layers.11.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.11.intermediate.dense.weight赋值给bert_encoder.bert_layers.11.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.11.intermediate.dense.bias赋值给bert_encoder.bert_layers.11.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.11.output.dense.weight赋值给bert_encoder.bert_layers.11.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.11.output.dense.bias赋值给bert_encoder.bert_layers.11.bert_output.dense.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.11.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.11.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.encoder.layer.11.output.LayerNorm.beta赋值给bert_encoder.bert_layers.11.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.pooler.dense.weight赋值给bert_pooler.dense.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:20] - DEBUG: ## 成功将参数:bert.pooler.dense.bias赋值给bert_pooler.dense.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:20] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
[2025-03-03 09:20:20] - INFO: ## 使用token embedding中的权重矩阵作为输出层的权重！torch.Size([21128, 768])
[2025-03-03 09:20:22] - INFO: ## 成功载入已有模型进行推理......
[2025-03-03 09:20:22] - INFO:  ### 原始: 十年生死两茫茫。不思量。自难忘。千里孤坟，无处话凄凉。
[2025-03-03 09:20:22] - INFO:   ## 掩盖: 十年生[MASK]两茫[MASK]。不思量[MASK]自难忘。千里孤坟[MASK]无处话凄凉。
[2025-03-03 09:20:22] - INFO:   ## 预测: 十年生死两茫茫。不思量，自难忘。千里孤坟，无处话凄凉。
[2025-03-03 09:20:22] - INFO: ===============
[2025-03-03 09:20:22] - INFO:  ### 原始: 红酥手。黄藤酒。满园春色宫墙柳。
[2025-03-03 09:20:22] - INFO:   ## 掩盖: 红[MASK]手。黄藤酒。满园春色宫墙[MASK]。
[2025-03-03 09:20:22] - INFO:   ## 预测: 红袖手。黄藤酒。满园春色宫墙侧。
[2025-03-03 09:20:22] - INFO: ===============
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.embeddings.word_embeddings.weight赋值给bert_embeddings.word_embeddings.embedding.weight,参数形状为:torch.Size([21128, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.embeddings.position_embeddings.weight赋值给bert_embeddings.position_embeddings.embedding.weight,参数形状为:torch.Size([512, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.embeddings.token_type_embeddings.weight赋值给bert_embeddings.token_type_embeddings.embedding.weight,参数形状为:torch.Size([2, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.embeddings.LayerNorm.gamma赋值给bert_embeddings.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.embeddings.LayerNorm.beta赋值给bert_embeddings.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.self.query.weight赋值给bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.self.query.bias赋值给bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.self.key.weight赋值给bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.self.key.bias赋值给bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.self.value.weight赋值给bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.self.value.bias赋值给bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.output.dense.weight赋值给bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.output.dense.bias赋值给bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.0.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.output.LayerNorm.beta赋值给bert_encoder.bert_layers.0.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.0.intermediate.dense.weight赋值给bert_encoder.bert_layers.0.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.0.intermediate.dense.bias赋值给bert_encoder.bert_layers.0.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.0.output.dense.weight赋值给bert_encoder.bert_layers.0.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.0.output.dense.bias赋值给bert_encoder.bert_layers.0.bert_output.dense.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.0.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.0.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.0.output.LayerNorm.beta赋值给bert_encoder.bert_layers.0.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.self.query.weight赋值给bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.self.query.bias赋值给bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.self.key.weight赋值给bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.self.key.bias赋值给bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.self.value.weight赋值给bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.self.value.bias赋值给bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.output.dense.weight赋值给bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.output.dense.bias赋值给bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.1.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.output.LayerNorm.beta赋值给bert_encoder.bert_layers.1.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.1.intermediate.dense.weight赋值给bert_encoder.bert_layers.1.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.1.intermediate.dense.bias赋值给bert_encoder.bert_layers.1.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.1.output.dense.weight赋值给bert_encoder.bert_layers.1.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.1.output.dense.bias赋值给bert_encoder.bert_layers.1.bert_output.dense.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.1.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.1.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.1.output.LayerNorm.beta赋值给bert_encoder.bert_layers.1.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.self.query.weight赋值给bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.self.query.bias赋值给bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.self.key.weight赋值给bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.self.key.bias赋值给bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.self.value.weight赋值给bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.self.value.bias赋值给bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.output.dense.weight赋值给bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.output.dense.bias赋值给bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.2.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.output.LayerNorm.beta赋值给bert_encoder.bert_layers.2.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.2.intermediate.dense.weight赋值给bert_encoder.bert_layers.2.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.2.intermediate.dense.bias赋值给bert_encoder.bert_layers.2.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.2.output.dense.weight赋值给bert_encoder.bert_layers.2.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.2.output.dense.bias赋值给bert_encoder.bert_layers.2.bert_output.dense.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.2.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.2.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.2.output.LayerNorm.beta赋值给bert_encoder.bert_layers.2.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.self.query.weight赋值给bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.self.query.bias赋值给bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.self.key.weight赋值给bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.self.key.bias赋值给bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.self.value.weight赋值给bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.self.value.bias赋值给bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.output.dense.weight赋值给bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.output.dense.bias赋值给bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.3.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.output.LayerNorm.beta赋值给bert_encoder.bert_layers.3.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.3.intermediate.dense.weight赋值给bert_encoder.bert_layers.3.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.3.intermediate.dense.bias赋值给bert_encoder.bert_layers.3.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.3.output.dense.weight赋值给bert_encoder.bert_layers.3.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.3.output.dense.bias赋值给bert_encoder.bert_layers.3.bert_output.dense.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.3.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.3.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.3.output.LayerNorm.beta赋值给bert_encoder.bert_layers.3.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.self.query.weight赋值给bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.self.query.bias赋值给bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.self.key.weight赋值给bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.self.key.bias赋值给bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.self.value.weight赋值给bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.self.value.bias赋值给bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.output.dense.weight赋值给bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.output.dense.bias赋值给bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.4.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.output.LayerNorm.beta赋值给bert_encoder.bert_layers.4.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.4.intermediate.dense.weight赋值给bert_encoder.bert_layers.4.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.4.intermediate.dense.bias赋值给bert_encoder.bert_layers.4.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.4.output.dense.weight赋值给bert_encoder.bert_layers.4.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.4.output.dense.bias赋值给bert_encoder.bert_layers.4.bert_output.dense.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.4.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.4.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.4.output.LayerNorm.beta赋值给bert_encoder.bert_layers.4.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.self.query.weight赋值给bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.self.query.bias赋值给bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.self.key.weight赋值给bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.self.key.bias赋值给bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.self.value.weight赋值给bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.self.value.bias赋值给bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.output.dense.weight赋值给bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.output.dense.bias赋值给bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.5.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.output.LayerNorm.beta赋值给bert_encoder.bert_layers.5.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.5.intermediate.dense.weight赋值给bert_encoder.bert_layers.5.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.5.intermediate.dense.bias赋值给bert_encoder.bert_layers.5.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.5.output.dense.weight赋值给bert_encoder.bert_layers.5.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.5.output.dense.bias赋值给bert_encoder.bert_layers.5.bert_output.dense.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.5.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.5.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.5.output.LayerNorm.beta赋值给bert_encoder.bert_layers.5.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.self.query.weight赋值给bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.self.query.bias赋值给bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.self.key.weight赋值给bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.self.key.bias赋值给bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.self.value.weight赋值给bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.self.value.bias赋值给bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.output.dense.weight赋值给bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.output.dense.bias赋值给bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.6.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.output.LayerNorm.beta赋值给bert_encoder.bert_layers.6.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.6.intermediate.dense.weight赋值给bert_encoder.bert_layers.6.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.6.intermediate.dense.bias赋值给bert_encoder.bert_layers.6.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.6.output.dense.weight赋值给bert_encoder.bert_layers.6.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.6.output.dense.bias赋值给bert_encoder.bert_layers.6.bert_output.dense.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.6.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.6.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.6.output.LayerNorm.beta赋值给bert_encoder.bert_layers.6.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.self.query.weight赋值给bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.self.query.bias赋值给bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.self.key.weight赋值给bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.self.key.bias赋值给bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.self.value.weight赋值给bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.self.value.bias赋值给bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.output.dense.weight赋值给bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.output.dense.bias赋值给bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.7.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.output.LayerNorm.beta赋值给bert_encoder.bert_layers.7.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.7.intermediate.dense.weight赋值给bert_encoder.bert_layers.7.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.7.intermediate.dense.bias赋值给bert_encoder.bert_layers.7.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.7.output.dense.weight赋值给bert_encoder.bert_layers.7.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.7.output.dense.bias赋值给bert_encoder.bert_layers.7.bert_output.dense.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.7.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.7.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.7.output.LayerNorm.beta赋值给bert_encoder.bert_layers.7.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.self.query.weight赋值给bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.self.query.bias赋值给bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.self.key.weight赋值给bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.self.key.bias赋值给bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.self.value.weight赋值给bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.self.value.bias赋值给bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.output.dense.weight赋值给bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.output.dense.bias赋值给bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.8.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.output.LayerNorm.beta赋值给bert_encoder.bert_layers.8.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.8.intermediate.dense.weight赋值给bert_encoder.bert_layers.8.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.8.intermediate.dense.bias赋值给bert_encoder.bert_layers.8.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.8.output.dense.weight赋值给bert_encoder.bert_layers.8.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.8.output.dense.bias赋值给bert_encoder.bert_layers.8.bert_output.dense.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.8.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.8.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.8.output.LayerNorm.beta赋值给bert_encoder.bert_layers.8.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.self.query.weight赋值给bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.self.query.bias赋值给bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.self.key.weight赋值给bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.self.key.bias赋值给bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.self.value.weight赋值给bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.self.value.bias赋值给bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.output.dense.weight赋值给bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.output.dense.bias赋值给bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.9.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.output.LayerNorm.beta赋值给bert_encoder.bert_layers.9.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.9.intermediate.dense.weight赋值给bert_encoder.bert_layers.9.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.9.intermediate.dense.bias赋值给bert_encoder.bert_layers.9.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.9.output.dense.weight赋值给bert_encoder.bert_layers.9.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.9.output.dense.bias赋值给bert_encoder.bert_layers.9.bert_output.dense.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.9.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.9.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.9.output.LayerNorm.beta赋值给bert_encoder.bert_layers.9.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.self.query.weight赋值给bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.self.query.bias赋值给bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.self.key.weight赋值给bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.self.key.bias赋值给bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.self.value.weight赋值给bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.self.value.bias赋值给bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.output.dense.weight赋值给bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.output.dense.bias赋值给bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.10.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.output.LayerNorm.beta赋值给bert_encoder.bert_layers.10.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.10.intermediate.dense.weight赋值给bert_encoder.bert_layers.10.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.10.intermediate.dense.bias赋值给bert_encoder.bert_layers.10.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.10.output.dense.weight赋值给bert_encoder.bert_layers.10.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.10.output.dense.bias赋值给bert_encoder.bert_layers.10.bert_output.dense.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.10.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.10.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.10.output.LayerNorm.beta赋值给bert_encoder.bert_layers.10.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.self.query.weight赋值给bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.self.query.bias赋值给bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.self.key.weight赋值给bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.self.key.bias赋值给bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.self.value.weight赋值给bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.self.value.bias赋值给bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.output.dense.weight赋值给bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.output.dense.bias赋值给bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.11.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.output.LayerNorm.beta赋值给bert_encoder.bert_layers.11.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.11.intermediate.dense.weight赋值给bert_encoder.bert_layers.11.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.11.intermediate.dense.bias赋值给bert_encoder.bert_layers.11.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.11.output.dense.weight赋值给bert_encoder.bert_layers.11.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.11.output.dense.bias赋值给bert_encoder.bert_layers.11.bert_output.dense.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.11.output.LayerNorm.gamma赋值给bert_encoder.bert_layers.11.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.encoder.layer.11.output.LayerNorm.beta赋值给bert_encoder.bert_layers.11.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.pooler.dense.weight赋值给bert_pooler.dense.weight,参数形状为:torch.Size([768, 768])
[2025-03-03 09:20:25] - DEBUG: ## 成功将参数:bert.pooler.dense.bias赋值给bert_pooler.dense.bias,参数形状为:torch.Size([768])
[2025-03-03 09:20:25] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
[2025-03-03 09:20:25] - INFO: ## 使用token embedding中的权重矩阵作为输出层的权重！torch.Size([21128, 768])
[2025-03-03 09:20:25] - INFO: ## 成功载入已有模型进行推理......
[2025-03-03 09:20:25] - INFO:  ### 原始: 我住长江头，君住长江尾。
[2025-03-03 09:20:25] - INFO:   ## 掩盖: 我[MASK]长江头[MASK]君住长江尾。
[2025-03-03 09:20:25] - INFO:   ## 预测: 我住长江头，君住长江尾。
[2025-03-03 09:20:25] - INFO: ===============
[2025-03-03 09:20:25] - INFO:  ### 原始: 日日思君不见君，共饮长江水。
[2025-03-03 09:20:25] - INFO:   ## 掩盖: 日日思[MASK]不见君，共[MASK]长江水。
[2025-03-03 09:20:25] - INFO:   ## 预测: 日日思君不见君，共酌长江水。
[2025-03-03 09:20:25] - INFO: ===============
[2025-03-03 09:20:25] - INFO:  ### 原始: 此水几时休，此恨何时已。
[2025-03-03 09:20:25] - INFO:   ## 掩盖: 此[MASK]几时休[MASK]此恨何时已。
[2025-03-03 09:20:25] - INFO:   ## 预测: 此恨几时休，此恨何时已。
[2025-03-03 09:20:25] - INFO: ===============
[2025-03-03 09:20:25] - INFO:  ### 原始: 只愿君心似我心，定不负相思意。
[2025-03-03 09:20:25] - INFO:   ## 掩盖: 只愿君[MASK]似我心，定不[MASK]相思意。
[2025-03-03 09:20:25] - INFO:   ## 预测: 只愿君心似我心，定不会相思意。
[2025-03-03 09:20:25] - INFO: ===============
